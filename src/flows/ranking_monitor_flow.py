"""
ë­í‚¹ ëª¨ë‹ˆí„°ë§ Flow - 6ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰
Amazon ë­í‚¹ì„ ìˆ˜ì§‘í•˜ê³  ì´ë²¤íŠ¸ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
"""
from prefect import flow, task
from prefect.artifacts import create_markdown_artifact, create_table_artifact
from datetime import timedelta
from loguru import logger

from src.core.database import get_db_context
from src.analyzers.event_detector import EventDetector
from src.tasks.scraping_tasks import scrape_all_amazon_categories_task
from src.tasks.processing_tasks import save_amazon_rankings_task
from src.flows.context_collection_flow import context_collection_flow
from src.flows.insight_generation_flow import insight_generation_flow
from config.settings import settings


@task(name="detect_ranking_events")
def detect_ranking_events_task():
    """ë­í‚¹ ì´ë²¤íŠ¸ ê°ì§€"""

    detector = EventDetector(
        rank_change_threshold=settings.EVENT_RANK_CHANGE_THRESHOLD,
        rank_change_pct_threshold=settings.EVENT_RANK_CHANGE_PCT_THRESHOLD,
        price_change_pct_threshold=settings.EVENT_PRICE_CHANGE_PCT_THRESHOLD,
        review_surge_threshold=settings.EVENT_REVIEW_SURGE_THRESHOLD
    )

    with get_db_context() as db:
        events = detector.detect_events(db, lookback_hours=12)

        # ì´ë²¤íŠ¸ DBì— ì €ì¥
        for event in events:
            db.add(event)

        db.commit()

        logger.info(f"ì´ {len(events)}ê°œ ì´ë²¤íŠ¸ ì €ì¥ ì™„ë£Œ")

        return events


@task(name="create_event_summary_artifact")
async def create_event_summary_artifact_task(events):
    """ì´ë²¤íŠ¸ ìš”ì•½ Artifact ìƒì„±"""

    if not events:
        await create_markdown_artifact(
            key="ranking-events-summary",
            markdown="## ë­í‚¹ ì´ë²¤íŠ¸\n\nì´ë²ˆ ì£¼ê¸°ì—ëŠ” ê°ì§€ëœ ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.",
            description="ë­í‚¹ ë³€ë™ ì´ë²¤íŠ¸ ìš”ì•½"
        )
        return

    # Markdown ìš”ì•½ ìƒì„±
    summary_md = "## ğŸ¯ ë­í‚¹ ì´ë²¤íŠ¸ ê°ì§€ ê²°ê³¼\n\n"
    summary_md += f"**ì´ {len(events)}ê°œ ì´ë²¤íŠ¸ ê°ì§€**\n\n"

    # ì‹¬ê°ë„ë³„ ë¶„ë¥˜
    by_severity = {}
    for event in events:
        severity = event.severity
        if severity not in by_severity:
            by_severity[severity] = []
        by_severity[severity].append(event)

    for severity in ['critical', 'high', 'medium', 'low']:
        if severity in by_severity:
            count = len(by_severity[severity])
            emoji = {'critical': 'ğŸ”´', 'high': 'ğŸŸ ', 'medium': 'ğŸŸ¡', 'low': 'ğŸŸ¢'}[severity]
            summary_md += f"- {emoji} **{severity.upper()}**: {count}ê°œ\n"

    summary_md += "\n### ìƒì„¸ ë‚´ì—­\n\n"

    for event in events[:10]:  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
        summary_md += f"#### {event.event_type}\n"
        summary_md += f"- **ì œí’ˆ ID**: {event.product_id}\n"
        summary_md += f"- **ì‹¬ê°ë„**: {event.severity}\n"

        if event.rank_change:
            summary_md += f"- **ë­í‚¹ ë³€ë™**: {event.prev_rank}ìœ„ â†’ {event.curr_rank}ìœ„ ({event.rank_change:+d})\n"

        if event.price_change_pct:
            summary_md += f"- **ê°€ê²© ë³€ë™**: {event.price_change_pct:+.1f}%\n"

        summary_md += f"- **ê°ì§€ ì‹œê°„**: {event.detected_at}\n\n"

    await create_markdown_artifact(
        key="ranking-events-summary",
        markdown=summary_md,
        description=f"ë­í‚¹ ë³€ë™ ì´ë²¤íŠ¸ ìš”ì•½ ({len(events)}ê°œ)"
    )

    # í…Œì´ë¸” Artifact
    table_data = []
    for event in events:
        table_data.append({
            "Event Type": event.event_type,
            "Severity": event.severity,
            "Product ID": event.product_id,
            "Rank Change": f"{event.prev_rank}â†’{event.curr_rank}" if event.rank_change else "-",
            "Detected At": event.detected_at.strftime('%Y-%m-%d %H:%M')
        })

    await create_table_artifact(
        key="ranking-events-table",
        table=table_data,
        description="ì´ë²¤íŠ¸ ëª©ë¡"
    )


@flow(
    name="ranking_monitor",
    description="Amazon ë­í‚¹ ëª¨ë‹ˆí„°ë§ ë° ì´ë²¤íŠ¸ ê°ì§€"
)
async def ranking_monitor_flow():
    """
    ë©”ì¸ ë­í‚¹ ëª¨ë‹ˆí„°ë§ Flow

    1. Amazon ë­í‚¹ ìˆ˜ì§‘
    2. ì´ë²¤íŠ¸ ê°ì§€
    3. Critical/High ì´ë²¤íŠ¸ ë°œê²¬ ì‹œ Context Collection Flow íŠ¸ë¦¬ê±°
    4. Context ìˆ˜ì§‘ ì™„ë£Œ í›„ Insight Generation Flow íŠ¸ë¦¬ê±°
    """

    logger.info("=" * 80)
    logger.info("Ranking Monitor Flow ì‹œì‘")
    logger.info("=" * 80)

    # 1. Amazon ë­í‚¹ ìˆ˜ì§‘
    logger.info("Step 1: Amazon ë­í‚¹ ìˆ˜ì§‘")
    ranking_results = await scrape_all_amazon_categories_task()

    # 2. DB ì €ì¥
    logger.info("Step 2: ë­í‚¹ ë°ì´í„° DB ì €ì¥")
    await save_amazon_rankings_task(ranking_results)

    # 3. ì´ë²¤íŠ¸ ê°ì§€
    logger.info("Step 3: ì´ë²¤íŠ¸ ê°ì§€")
    events = await detect_ranking_events_task()

    # 4. Artifact ìƒì„±
    logger.info("Step 4: Artifact ìƒì„±")
    await create_event_summary_artifact_task(events)

    # 5. ì´ë²¤íŠ¸ ë°œê²¬ ì‹œ Context Collection Flow íŠ¸ë¦¬ê±°
    context_flows_triggered = 0
    if events:
        logger.info("Step 5: Context Collection Flow íŠ¸ë¦¬ê±°")
        for event in events:
            # Critical ë˜ëŠ” High ì‹¬ê°ë„ ì´ë²¤íŠ¸ë§Œ ì²˜ë¦¬
            if event.severity in ['critical', 'high']:
                logger.info(f"  - Event ID {event.id} ({event.event_type}, {event.severity}) ì²˜ë¦¬ ì‹œì‘")
                try:
                    context_collection_flow(event.id)
                    context_flows_triggered += 1
                except Exception as e:
                    logger.error(f"  - Event ID {event.id} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

        logger.info(f"ì´ {context_flows_triggered}ê°œ Context Collection Flow ì‹¤í–‰")

    # 6. ì¸ì‚¬ì´íŠ¸ ìƒì„± Flow íŠ¸ë¦¬ê±° (Context ìˆ˜ì§‘ì´ ì™„ë£Œëœ ì´ë²¤íŠ¸ ëŒ€ìƒ)
    insight_result = None
    if context_flows_triggered > 0:
        logger.info("Step 6: Insight Generation Flow íŠ¸ë¦¬ê±°")
        try:
            # Context ìˆ˜ì§‘ì´ ì™„ë£Œëœ ì´ë²¤íŠ¸ë“¤ì— ëŒ€í•´ ì¸ì‚¬ì´íŠ¸ ìƒì„±
            insight_result = await insight_generation_flow(limit=10)
            logger.info(f"  - ì¸ì‚¬ì´íŠ¸ ìƒì„±: {insight_result.get('successful', 0)}ê°œ ì„±ê³µ")
        except Exception as e:
            logger.error(f"  - Insight Generation Flow ì‹¤íŒ¨: {e}")

    logger.info("=" * 80)
    logger.info(f"Ranking Monitor Flow ì™„ë£Œ - {len(events)}ê°œ ì´ë²¤íŠ¸ ê°ì§€")
    logger.info(f"  - Context ìˆ˜ì§‘: {context_flows_triggered}ê°œ")
    if insight_result:
        logger.info(f"  - ì¸ì‚¬ì´íŠ¸ ìƒì„±: {insight_result.get('successful', 0)}ê°œ")
    logger.info("=" * 80)

    return {
        'total_categories': len(ranking_results) if ranking_results else 0,
        'events_detected': len(events),
        'context_flows_triggered': context_flows_triggered,
        'insights_generated': insight_result.get('successful', 0) if insight_result else 0,
        'events': [e.to_dict() for e in events]
    }


# Prefect Deployment
if __name__ == "__main__":
    from prefect import serve

    # 6ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰
    ranking_monitor_deployment = ranking_monitor_flow.to_deployment(
        name="ranking-monitor-6h",
        interval=timedelta(hours=6),
        tags=["amazon", "ranking", "event-detection"]
    )

    serve(ranking_monitor_deployment)
